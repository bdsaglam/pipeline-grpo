{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# See https://github.com/willccbb/verifiers for ongoing developments\n",
    "#\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f32823b9424a1abe5844c931ef132d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0662f786b1a14602962f912fccc84646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbc16b4783a4990a33a9090fcecae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8187364604749a1b62a1930e079b53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d22054f2e64496a147a9147de0dfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "WandbCallback requires wandb to be installed. Run `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mpad_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39meos_token\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# use peft at your own risk; not working for me with multi-GPU training\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m trainer \u001b[39m=\u001b[39m GRPOTrainer(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m     processing_class\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m     reward_funcs\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m         xmlcount_reward_func,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m         soft_format_reward_func,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m         strict_format_reward_func,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m         int_reward_func,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m         correctness_reward_func],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m     \u001b[39m#peft_config=peft_config\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:330\u001b[0m, in \u001b[0;36mGRPOTrainer.__init__\u001b[0;34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metrics \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_completions \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mlog_completions\n\u001b[0;32m--> 330\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    331\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    332\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    333\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m    334\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m    335\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[1;32m    336\u001b[0m     processing_class\u001b[39m=\u001b[39;49mprocessing_class,\n\u001b[1;32m    337\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    338\u001b[0m     optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[1;32m    339\u001b[0m )\n\u001b[1;32m    341\u001b[0m \u001b[39m# Check if the per_device_train/eval_batch_size * num processes can be divided by the number of generations\u001b[39;00m\n\u001b[1;32m    342\u001b[0m num_processes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mnum_processes\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m minimum_action \u001b[39min\u001b[39;00m (Action\u001b[39m.\u001b[39mNOTIFY, Action\u001b[39m.\u001b[39mNOTIFY_ALWAYS) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[39m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/transformers/trainer.py:677\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    675\u001b[0m default_callbacks \u001b[39m=\u001b[39m DEFAULT_CALLBACKS \u001b[39m+\u001b[39m get_reporting_integration_callbacks(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mreport_to)\n\u001b[1;32m    676\u001b[0m callbacks \u001b[39m=\u001b[39m default_callbacks \u001b[39mif\u001b[39;00m callbacks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m default_callbacks \u001b[39m+\u001b[39m callbacks\n\u001b[0;32m--> 677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler \u001b[39m=\u001b[39m CallbackHandler(\n\u001b[1;32m    678\u001b[0m     callbacks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessing_class, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler\n\u001b[1;32m    679\u001b[0m )\n\u001b[1;32m    680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_callback(PrinterCallback \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdisable_tqdm \u001b[39melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[1;32m    682\u001b[0m \u001b[39m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:450\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[0;34m(self, callbacks, model, processing_class, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m    449\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks:\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_callback(cb)\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m    452\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessing_class \u001b[39m=\u001b[39m processing_class\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:467\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39madd_callback\u001b[39m(\u001b[39mself\u001b[39m, callback):\n\u001b[0;32m--> 467\u001b[0m     cb \u001b[39m=\u001b[39m callback() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\n\u001b[1;32m    468\u001b[0m     cb_class \u001b[39m=\u001b[39m callback \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mif\u001b[39;00m cb_class \u001b[39min\u001b[39;00m [c\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks]:\n",
      "File \u001b[0;32m~/repos/pipeline-grpo/.venv/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:773\u001b[0m, in \u001b[0;36mWandbCallback.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m has_wandb \u001b[39m=\u001b[39m is_wandb_available()\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_wandb:\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWandbCallback requires wandb to be installed. Run `pip install wandb`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m has_wandb:\n\u001b[1;32m    775\u001b[0m     \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwandb\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: WandbCallback requires wandb to be installed. Run `pip install wandb`."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prep dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            #{'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
    "            #{'role': 'assistant', 'content': XML_COT_FORMAT.format(\n",
    "            #    reasoning=\"9 is divisble by 3 and 8 is divisible by 2, but 7 is prime.\",\n",
    "            #    answer=\"7\"\n",
    "            #)},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "#model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "if \"Llama\" in model_name:\n",
    "    output_dir = \"outputs/Llama-1B-GRPO\"\n",
    "    run_name = \"Llama-1B-GRPO-gsm8k\"\n",
    "else:\n",
    "    output_dir=\"outputs/Qwen-1.5B-GRPO\"\n",
    "    run_name=\"Qwen-1.5B-GRPO-gsm8k\"\n",
    "    \n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=786,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# use peft at your own risk; not working for me with multi-GPU training\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    #peft_config=peft_config\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
