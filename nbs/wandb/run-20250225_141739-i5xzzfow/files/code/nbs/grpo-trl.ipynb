{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set LD_LIBRARY_PATH for NVIDIA JIT Link\n",
    "# import os\n",
    "\n",
    "# # Get current LD_LIBRARY_PATH\n",
    "# current_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "# # Add NVIDIA JIT Link path\n",
    "# nvidia_path = \"/home/baris/miniconda3/envs/grpo/lib/python3.12/site-packages/nvidia/nvjitlink/lib\"\n",
    "\n",
    "# # Set the updated path\n",
    "# if nvidia_path not in current_path:\n",
    "#     os.environ[\"LD_LIBRARY_PATH\"] = f\"{nvidia_path}:{current_path}\"\n",
    "\n",
    "# print(f\"LD_LIBRARY_PATH set to: {os.environ['LD_LIBRARY_PATH']}\")\n",
    "\n",
    "# # Note: This only affects the current notebook session\n",
    "# # For a permanent solution, add this to your .bashrc or .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# See https://github.com/willccbb/verifiers for ongoing developments\n",
    "#\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d757b31c7ea4b428072598b9cac94e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbdsaglam\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baris/repos/pipeline-grpo/nbs/wandb/run-20250225_141519-n0xr76i9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bdsaglam/huggingface/runs/n0xr76i9' target=\"_blank\">Qwen-1.5B-GRPO-gsm8k</a></strong> to <a href='https://wandb.ai/bdsaglam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bdsaglam/huggingface' target=\"_blank\">https://wandb.ai/bdsaglam/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bdsaglam/huggingface/runs/n0xr76i9' target=\"_blank\">https://wandb.ai/bdsaglam/huggingface/runs/n0xr76i9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers? \n",
      "Answer:\n",
      "100 \n",
      "Response:\n",
      "To determine the minimum grade Ahmed needs to beat Emily, we first need to calculate the total class grade based on the assignments completed so far and the final assignment grades.\n",
      "\n",
      "1. **Current class grade calculation for Ahmed:**\n",
      "   Ahmed has 8 assignments and a current grade of 91 out of 100.\n",
      "   \\[\n",
      "   \\text{Current class grade for Ahmed} = 8 \\times 91/100 = 72.8\n",
      "   \\]\n",
      "\n",
      "2. **Present total grade for Emily:**\n",
      "   Emily has her current grade of 92 out of 100 and the final assignment grade of 90.\n",
      "   \\[\n",
      "   \\text{Present total grade for Emily} = 92 + 90 = 182\n",
      "   \\]\n",
      "\n",
      "3. **Minimum possible grade Emily can get:**\n",
      "   Since Emily's final assignment is worth the same amount as all other assignments, her final grade will be:\n",
      "   \\[\n",
      "   \\text{Final grade for Emily} = \\frac{182 + 90}{2} = \\frac{272}{2} = 136\n",
      "   \\]\n",
      "\n",
      "4. **To beat Emily, Ahmed needs to have a class grade of at least 137:**\n",
      "   Ahmed's current grade is 72.8, so we need to find the minimum grade Ahmed can get on the final assignment.\n",
      "   \\[\n",
      "   \\text{Minimum required grade for Ahmed} = 137 - 72.8 = 64.2\n",
      "   \\]\n",
      "\n",
      "Therefore, Ahmed needs to get at least a 65 on the final assignment to beat Emily. \n",
      "Extracted:\n",
      "To determine the minimum grade Ahmed needs to beat Emily, we first need to calculate the total class grade based on the assignments completed so far and the final assignment grades.\n",
      "\n",
      "1. **Current class grade calculation for Ahmed:**\n",
      "   Ahmed has 8 assignments and a current grade of 91 out of 100.\n",
      "   \\[\n",
      "   \\text{Current class grade for Ahmed} = 8 \\times 91/100 = 72.8\n",
      "   \\]\n",
      "\n",
      "2. **Present total grade for Emily:**\n",
      "   Emily has her current grade of 92 out of 100 and the final assignment grade of 90.\n",
      "   \\[\n",
      "   \\text{Present total grade for Emily} = 92 + 90 = 182\n",
      "   \\]\n",
      "\n",
      "3. **Minimum possible grade Emily can get:**\n",
      "   Since Emily's final assignment is worth the same amount as all other assignments, her final grade will be:\n",
      "   \\[\n",
      "   \\text{Final grade for Emily} = \\frac{182 + 90}{2} = \\frac{272}{2} = 136\n",
      "   \\]\n",
      "\n",
      "4. **To beat Emily, Ahmed needs to have a class grade of at least 137:**\n",
      "   Ahmed's current grade is 72.8, so we need to find the minimum grade Ahmed can get on the final assignment.\n",
      "   \\[\n",
      "   \\text{Minimum required grade for Ahmed} = 137 - 72.8 = 64.2\n",
      "   \\]\n",
      "\n",
      "Therefore, Ahmed needs to get at least a 65 on the final assignment to beat Emily.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='7473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/7473 00:28 < 20:00:29, 0.10 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "The gauge on a water tank shows that the tank is 1/3 full of water. To fill the tank, 16 gallons of water are added. How many gallons of water does the tank hold when full? \n",
      "Answer:\n",
      "24 \n",
      "Response:\n",
      "To determine the total capacity of the tank when it is full, we can use the information provided about the current state of the tank and the amount of water needed to fill it.\n",
      "\n",
      "Given:\n",
      "- The gauge shows the tank is 1/3 full.\n",
      "- 16 gallons of water are added to fill the tank.\n",
      "\n",
      "Let's denote:\n",
      "- The total capacity of the tank as \\( x \\) gallons.\n",
      "\n",
      "Since the gauge shows that the tank is 1/3 full, the current amount of water in the tank is \\( \\frac{1}{3}x \\).\n",
      "\n",
      "The amount of water added to fill the tank is \\( 16 \\) gallons. Therefore, we can set up the following equation:\n",
      "\\[\n",
      "\\frac{1}{3}x + 16 = x\n",
      "\\]\n",
      "\n",
      "To solve for \\( x \\), we first isolate the terms involving \\( x \\) on one side of the equation:\n",
      "\\[\n",
      "16 = x - \\frac{1}{3}x\n",
      "\\]\n",
      "\n",
      "Simplify \\( x - \\frac{1}{3}x \\):\n",
      "\\[\n",
      "x - \\frac{1}{3}x = \\frac{3}{3}x - \\frac{1}{3}x = \\frac{2}{3}x\n",
      "\\]\n",
      "\n",
      "So the equation becomes:\n",
      "\\[\n",
      "16 = \\frac{2}{3}x\n",
      "\\]\n",
      "\n",
      "To solve for \\( x \\), multiply both sides of the equation by \\( \\frac{3}{2} \\):\n",
      "\\[\n",
      "x = 16 \\times \\frac{3}{2}\n",
      "\\]\n",
      "\n",
      "Calculate the right side:\n",
      "\\[\n",
      "x = 24\n",
      "\\]\n",
      "\n",
      "Thus, the total capacity of the tank when it is full is:\n",
      "\\[\n",
      "\\boxed{24} \\text{ gallons}\n",
      "\\] \n",
      "Extracted:\n",
      "To determine the total capacity of the tank when it is full, we can use the information provided about the current state of the tank and the amount of water needed to fill it.\n",
      "\n",
      "Given:\n",
      "- The gauge shows the tank is 1/3 full.\n",
      "- 16 gallons of water are added to fill the tank.\n",
      "\n",
      "Let's denote:\n",
      "- The total capacity of the tank as \\( x \\) gallons.\n",
      "\n",
      "Since the gauge shows that the tank is 1/3 full, the current amount of water in the tank is \\( \\frac{1}{3}x \\).\n",
      "\n",
      "The amount of water added to fill the tank is \\( 16 \\) gallons. Therefore, we can set up the following equation:\n",
      "\\[\n",
      "\\frac{1}{3}x + 16 = x\n",
      "\\]\n",
      "\n",
      "To solve for \\( x \\), we first isolate the terms involving \\( x \\) on one side of the equation:\n",
      "\\[\n",
      "16 = x - \\frac{1}{3}x\n",
      "\\]\n",
      "\n",
      "Simplify \\( x - \\frac{1}{3}x \\):\n",
      "\\[\n",
      "x - \\frac{1}{3}x = \\frac{3}{3}x - \\frac{1}{3}x = \\frac{2}{3}x\n",
      "\\]\n",
      "\n",
      "So the equation becomes:\n",
      "\\[\n",
      "16 = \\frac{2}{3}x\n",
      "\\]\n",
      "\n",
      "To solve for \\( x \\), multiply both sides of the equation by \\( \\frac{3}{2} \\):\n",
      "\\[\n",
      "x = 16 \\times \\frac{3}{2}\n",
      "\\]\n",
      "\n",
      "Calculate the right side:\n",
      "\\[\n",
      "x = 24\n",
      "\\]\n",
      "\n",
      "Thus, the total capacity of the tank when it is full is:\n",
      "\\[\n",
      "\\boxed{24} \\text{ gallons}\n",
      "\\]\n",
      "-------------------- Question:\n",
      "There are 15 tables in the school's cafeteria. Each table can seat 10 people. Usually, only 1/10 of the seats are left unseated. How many seats are usually taken? \n",
      "Answer:\n",
      "135 \n",
      "Response:\n",
      "To find out how many seats are usually taken in the cafeteria, we can use the following reasoning steps:\n",
      "\n",
      "1. First, we need to determine the total number of seats available in the cafeteria. Since each table can seat 10 people and there are 15 tables, the total number of seats is:\n",
      "   15 tables * 10 people/table = 150 people\n",
      "\n",
      "2. Normally, only 1/10 of these seats are left unseated. So, 9/10 of the seats are usually taken because:\n",
      "   The number of seats usually taken = Total seats * (9/10)\n",
      "\n",
      "3. Now, let's calculate it:\n",
      "   150 people * (9/10) = 135 people\n",
      "\n",
      "Therefore, usually 135 seats are taken in the cafeteria.\n",
      "\n",
      "<answer>\n",
      "135\n",
      "</answer> \n",
      "Extracted:\n",
      "135\n",
      "-------------------- Question:\n",
      "Fiona completed 36 math questions in an hour. Shirley was able to complete twice as many math questions within that same time, and Kiana completed half of the sum of Fiona and Shirley's math questions. If they each did the same number of questions the following hour, how many math questions did all three girls complete in 2 hours? \n",
      "Answer:\n",
      "324 \n",
      "Response:\n",
      "To calculate the total number of math questions completed by Fiona, Shirley, and Kiana in two hours, we will first find out how many questions each girl completed in the first hour.\n",
      "\n",
      "First hour:\n",
      "Fiona: \\(36\\) questions.\n",
      "Shirley: \\(2 \\times 36\\) questions.\n",
      "Kiana: \\(\\frac{1}{2} \\times (36 + 2 \\times 36)\\) questions.\n",
      "\n",
      "Calculating for Shirley:\n",
      "\\(2 \\times 36 = 72\\) questions.\n",
      "\n",
      "Calculating for Kiana:\n",
      "\\[36 + 72 = 108\\]\n",
      "\\[\\frac{1}{2} \\times 108 = 54\\] questions.\n",
      "\n",
      "Now for the second hour:\n",
      "Each girl will do the same number of questions as they did the first hour.\n",
      "\n",
      "The total number of questions completed by all three girls within the two hours is the sum of the questions each completed in one hour plus the questions each completed in the second hour.\n",
      "\n",
      "Fiona’s total:\n",
      "\\(36 + 36 = 72\\) questions.\n",
      "\n",
      "Shirley’s total:\n",
      "\\(72 + 72 = 144\\) questions.\n",
      "\n",
      "Kiana’s total:\n",
      "\\(54 + 54 = 108\\) questions.\n",
      "\n",
      "Adding the totals from all three girls:\n",
      "\\(72 + 144 + 108 = 324\\) questions.\n",
      "\n",
      "Therefore, the answer is \\(324\\) questions. \n",
      "Extracted:\n",
      "To calculate the total number of math questions completed by Fiona, Shirley, and Kiana in two hours, we will first find out how many questions each girl completed in the first hour.\n",
      "\n",
      "First hour:\n",
      "Fiona: \\(36\\) questions.\n",
      "Shirley: \\(2 \\times 36\\) questions.\n",
      "Kiana: \\(\\frac{1}{2} \\times (36 + 2 \\times 36)\\) questions.\n",
      "\n",
      "Calculating for Shirley:\n",
      "\\(2 \\times 36 = 72\\) questions.\n",
      "\n",
      "Calculating for Kiana:\n",
      "\\[36 + 72 = 108\\]\n",
      "\\[\\frac{1}{2} \\times 108 = 54\\] questions.\n",
      "\n",
      "Now for the second hour:\n",
      "Each girl will do the same number of questions as they did the first hour.\n",
      "\n",
      "The total number of questions completed by all three girls within the two hours is the sum of the questions each completed in one hour plus the questions each completed in the second hour.\n",
      "\n",
      "Fiona’s total:\n",
      "\\(36 + 36 = 72\\) questions.\n",
      "\n",
      "Shirley’s total:\n",
      "\\(72 + 72 = 144\\) questions.\n",
      "\n",
      "Kiana’s total:\n",
      "\\(54 + 54 = 108\\) questions.\n",
      "\n",
      "Adding the totals from all three girls:\n",
      "\\(72 + 144 + 108 = 324\\) questions.\n",
      "\n",
      "Therefore, the answer is \\(324\\) questions.\n",
      "-------------------- Question:\n",
      "In a graveyard, there are 20 skeletons.  Half of these skeletons are adult women, and the remaining number are split evenly between adult men and children.  If an adult woman has 20 bones in their body, and a male has 5 more than this, and a child has half as many as an adult woman, how many bones are in the graveyard? \n",
      "Answer:\n",
      "375 \n",
      "Response:\n",
      "The number of adult women is 20/2 = 10.\n",
      "The number of adult men is also 10.\n",
      "The number of children is also 10.\n",
      "The number of bones an adult woman has is 20.\n",
      "The number of bones an adult man has is 20+5 = 25.\n",
      "The number of bones a child has is 0.5*20 = 10.\n",
      "Therefore, the total number of bones in the graveyard is 10*20+10*25+10*10 = 600.\n",
      "...\n",
      "<answer>\n",
      "600\n",
      "</answer> \n",
      "Extracted:\n",
      "600\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.12 GiB. GPU 0 has a total capacity of 79.15 GiB of which 6.12 GiB is free. Including non-PyTorch memory, this process has 73.01 GiB memory in use. Of the allocated memory 67.30 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# use peft at your own risk; not working for me with multi-GPU training\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m trainer \u001b[39m=\u001b[39m GRPOTrainer(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m     processing_class\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m     \u001b[39m#peft_config=peft_config\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbiltir-baris/home/baris/repos/pipeline-grpo/nbs/grpo-trl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2242\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2243\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2244\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2246\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2550\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mscale_wrt_gas\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3742\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/grpo/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.12 GiB. GPU 0 has a total capacity of 79.15 GiB of which 6.12 GiB is free. Including non-PyTorch memory, this process has 73.01 GiB memory in use. Of the allocated memory 67.30 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prep dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            #{'role': 'user', 'content': 'What is the largest single-digit prime number?'},\n",
    "            #{'role': 'assistant', 'content': XML_COT_FORMAT.format(\n",
    "            #    reasoning=\"9 is divisble by 3 and 8 is divisible by 2, but 7 is prime.\",\n",
    "            #    answer=\"7\"\n",
    "            #)},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses] \n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "#model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "if \"Llama\" in model_name:\n",
    "    output_dir = \"outputs/Llama-1B-GRPO\"\n",
    "    run_name = \"Llama-1B-GRPO-gsm8k\"\n",
    "else:\n",
    "    output_dir=\"outputs/Qwen-1.5B-GRPO\"\n",
    "    run_name=\"Qwen-1.5B-GRPO-gsm8k\"\n",
    "    \n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=786,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ").to(\"cuda\")\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# use peft at your own risk; not working for me with multi-GPU training\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    #peft_config=peft_config\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
